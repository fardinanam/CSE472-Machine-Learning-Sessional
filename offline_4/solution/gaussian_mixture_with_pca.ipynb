{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "class PCA:\n",
    "    def __init__(self, n_components=2):\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.data = data\n",
    "        self.mean = np.mean(data, axis=0)\n",
    "        self.std = np.std(data, axis=0)\n",
    "        data = (data - self.mean) / self.std\n",
    "        U, S, V = np.linalg.svd(data, full_matrices=False)\n",
    "        self.components = V.T[:, :self.n_components]\n",
    "\n",
    "    def transform(self, data):\n",
    "        data = (data - self.mean) / self.std\n",
    "        return np.dot(data, self.components)\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('../datasets/3D_data_points.txt', delimiter=',')\n",
    "\n",
    "data_2d = data\n",
    "if data.shape[1] > 2:\n",
    "    pca = PCA(n_components=2)\n",
    "    data_2d = pca.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot original and 2d data side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.scatter(data[:, 0], data[:, 1])\n",
    "ax1.set_title('Original data')\n",
    "ax2.scatter(data_2d[:, 0], data_2d[:, 1])\n",
    "ax2.set_title('2D data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "class GaussianMixture:\n",
    "    def __init__(self, n_components=1, tol=1e-5):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_components (int): The number of mixture components.\n",
    "        max_iter (int): The maximum number of iterations to perform.\n",
    "        tol (float): The convergence threshold.\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.tol = tol\n",
    "\n",
    "    def _initialize_parameters(self, data: np.array) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the model parameters.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data (np.array): The data to fit the model to.\n",
    "        \"\"\"\n",
    "        # Initialize the weights\n",
    "        self.weights = np.ones(self.n_components) / self.n_components\n",
    "\n",
    "        # Initialize the means by sampling from the data\n",
    "        random_rows = np.random.choice(data.shape[0], self.n_components)\n",
    "        self.means = data[random_rows]\n",
    "\n",
    "        # Initialize the covariance matrices as identity\n",
    "        self.covs = np.array([np.eye(data.shape[1])] * self.n_components)\n",
    "\n",
    "        # Initialize the log likelihood\n",
    "        self.log_likelihood = 0.\n",
    "\n",
    "    def _multivariate_gaussian(self, data: np.array, mu: np.array, cov: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Compute the multivariate Gaussian distribution.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data (np.array): The data to fit the model to.\n",
    "        mu (np.array): The mean vector.\n",
    "        cov (np.array): The covariance matrix.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        np.array: The probability density.\n",
    "        \"\"\"\n",
    "        # Compute the determinant and inverse of the covariance matrix\n",
    "        det = np.linalg.det(cov)\n",
    "        inv = np.linalg.inv(cov)\n",
    "\n",
    "        # Compute the exponent\n",
    "        exp = np.exp(-0.5 * np.sum((data - mu) @ inv * (data - mu), axis=1))\n",
    "\n",
    "        # Compute the multivariate Gaussian\n",
    "        return 1. / np.sqrt((2 * np.pi) ** data.shape[1] * det) * exp\n",
    "    \n",
    "    def _likelihoods(self, data: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Compute the likelihoods of the data.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data (np.array): The data to fit the model to.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        np.array: The likelihoods.\n",
    "        \"\"\"\n",
    "        # Compute the likelihoods of each component given the data\n",
    "        return np.array([self._multivariate_gaussian(data, mu, cov) for mu, cov in zip(self.means, self.covs)])\n",
    "    \n",
    "    def _log_likelihood(self, data: np.array) -> float:\n",
    "        \"\"\"\n",
    "        Compute the log likelihood of the data.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data (np.array): The data to fit the model to.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        float: The log likelihood.\n",
    "        \"\"\"\n",
    "        # Compute the likelihoods of each component given the data\n",
    "        likelihoods = self._likelihoods(data)\n",
    "        weighted_likelihoods = likelihoods * self.weights[:, np.newaxis]\n",
    "\n",
    "        # Compute the log likelihood\n",
    "        return np.log(weighted_likelihoods.sum(axis=0)).sum()\n",
    "\n",
    "    def _expectation(self, data: np.array) -> None:\n",
    "        \"\"\"\n",
    "        The expectation step of the EM algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data (np.array): The data to fit the model to.\n",
    "        \"\"\"\n",
    "        # Compute the likelihoods of each component given the data\n",
    "        likelihoods = self._likelihoods(data)\n",
    "\n",
    "        # Compute the responsibilities\n",
    "        numerator = likelihoods * self.weights[:, np.newaxis]\n",
    "        denominator = numerator.sum(axis=0)\n",
    "        self.responsibilities = numerator / denominator\n",
    "\n",
    "    def _maximization(self, data: np.array) -> None:\n",
    "        \"\"\"\n",
    "        The maximization step of the EM algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data (np.array): The data to fit the model to.\n",
    "        \"\"\"\n",
    "        # Compute the total responsibility for each component\n",
    "        responsibilities = self.responsibilities.sum(axis=1)\n",
    "\n",
    "        # Update the weights\n",
    "        self.weights = responsibilities / data.shape[0]\n",
    "\n",
    "        # Update the means\n",
    "        weighted_sum = self.responsibilities @ data\n",
    "        self.means = weighted_sum / responsibilities[:, np.newaxis]\n",
    "\n",
    "        # Update the covariance matrices\n",
    "        for i in range(self.n_components):\n",
    "            diff = data - self.means[i]\n",
    "            weighted_sum = self.responsibilities[i] * diff.T @ diff\n",
    "            self.covs[i] = weighted_sum / responsibilities[i] + np.eye(data.shape[1]) * 1e-6\n",
    "\n",
    "    def _converged(self, data: np.array) -> bool:\n",
    "        \"\"\"\n",
    "        Check for convergence.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data (np.array): The data to fit the model to.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        bool: Whether the model has converged.\n",
    "        \"\"\"\n",
    "        new_log_likelihood = self._log_likelihood(data)\n",
    "\n",
    "        previous_log_likelihood = self.log_likelihood\n",
    "        self.log_likelihood = new_log_likelihood\n",
    "\n",
    "        # Check for convergence\n",
    "        if abs(new_log_likelihood - previous_log_likelihood) < self.tol:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def fit(self, data: np.array) -> None:\n",
    "        \"\"\"\n",
    "        Fit a Gaussian mixture model to the data.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data (np.array): The data to fit the model to.\n",
    "        \"\"\"\n",
    "        # Initialize the parameters\n",
    "        self._initialize_parameters(data)\n",
    "\n",
    "        self.history = [(self.means.copy(), self.covs.copy())]\n",
    "\n",
    "        # Iterate until convergence or max iterations\n",
    "        while True:\n",
    "            # E-step\n",
    "            self._expectation(data)\n",
    "\n",
    "            # M-step\n",
    "            self._maximization(data)\n",
    "\n",
    "            # Store the current means and covs\n",
    "            self.history.append((self.means.copy(), self.covs.copy()))\n",
    "\n",
    "            # Check for convergence\n",
    "            if self._converged(data):\n",
    "                break\n",
    "        \n",
    "    def show(self, data: np.array, frames: int = 40) -> None:\n",
    "        \"\"\"\n",
    "        Show the convergence of the EM algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data (np.array): The data to fit the model to.\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        steps = np.linspace(0, len(self.history) - 1, frames, dtype=np.int32)\n",
    "\n",
    "        # Generate a unique color for each Gaussian\n",
    "        colors = plt.cm.rainbow(np.linspace(0, 1, len(self.means)))\n",
    "\n",
    "        def update(i):\n",
    "            ax.clear()\n",
    "            ax.scatter(data[:, 0], data[:, 1], s=10)\n",
    "            for mean, cov, color in zip(*self.history[steps[i]], colors):\n",
    "                eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "                order = eigvals.argsort()[::-1]\n",
    "                eigvals, eigvecs = eigvals[order], eigvecs[:, order]\n",
    "                vx, vy = eigvecs[:,0][0], eigvecs[:,0][1]\n",
    "                theta = np.arctan2(vy, vx)\n",
    "                for j in range(1, 4):\n",
    "                    ell = Ellipse(xy=(mean[0], mean[1]),\n",
    "                                width=np.sqrt(eigvals[0])*j*2, height=np.sqrt(eigvals[1])*j*2,\n",
    "                                angle=np.degrees(theta),\n",
    "                                facecolor='none', edgecolor=color)\n",
    "                    ax.add_artist(ell)\n",
    "\n",
    "        ani = animation.FuncAnimation(fig, update, frames=frames, repeat=True)\n",
    "        ani.save('gmm.gif', writer='imagemagick')\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, data: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Predict the labels for the data.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data (np.array): The data to fit the model to.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        np.array: The predicted labels.\n",
    "        \"\"\"\n",
    "        # Compute the likelihoods of each component given the data\n",
    "        likelihoods = self._likelihoods(data)\n",
    "        \n",
    "        # Return the index of the component with the largest likelihood\n",
    "        return likelihoods.argmax(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "best_gmms = []\n",
    "for k in tqdm(range(3, 8)):\n",
    "    trials = 5\n",
    "    best_gmm = None\n",
    "    best_log_likelihood = -np.inf\n",
    "    convergence_log_likelihoods = np.zeros(trials)\n",
    "\n",
    "    for i in range(trials):\n",
    "        gmm = GaussianMixture(n_components=k)\n",
    "        gmm.fit(data_2d)\n",
    "\n",
    "        if gmm.log_likelihood > best_log_likelihood:\n",
    "            best_log_likelihood = gmm.log_likelihood\n",
    "            best_gmm = gmm\n",
    "    \n",
    "    best_gmms.append(best_gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihoods = [best_gmm.log_likelihood for best_gmm in best_gmms]\n",
    "best_gmm = best_gmms[np.argmax(log_likelihoods)]\n",
    "\n",
    "best_gmm.show(data_2d, frames=60)\n",
    "\n",
    "plt.plot(range(3, 8), log_likelihoods)\n",
    "plt.show()\n",
    "\n",
    "# choose an appropriate value of K (denoted by K’) based on the convergence log-likelihood.\n",
    "# plot the estimated GMM for K’ by showing sample data points and Gaussian distributions in a 2D plot.\n",
    "print(f'Clusters: {best_gmm.n_components}')\n",
    "print(f'Log likelihood: {best_gmm.log_likelihood}')\n",
    "\n",
    "# plot the original data points colored by their cluster assignments\n",
    "clusters = best_gmm.predict(data_2d)\n",
    "\n",
    "plt.scatter(data_2d[:, 0], data_2d[:, 1], c=clusters, s=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
